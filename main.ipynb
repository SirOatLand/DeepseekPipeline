{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from os import walk, stat, listdir, path, environ, remove\n",
    "from shutil import copy\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from codes.prep_onto import prep_onto\n",
    "from codes.prompt_API import create_news, create_newsdict, generate_onto\n",
    "from UnifiedOntologyPipeline.pipeline.appendOnto import append_file, process_lines\n",
    "from UnifiedOntologyPipeline.pipeline.connectIndividuals import getNamespace, connectIndividuals, clear_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"./codes/.env\")\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = environ.get(\"APIKEY\")\n",
    "URL = \"https://api.deepseek.com\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "START_IDX = int(environ.get(\"NEWS_START\"))\n",
    "END_IDX = int(environ.get(\"NEWS_END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **News Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = \"./codes/TheHackerNews_Dataset.xlsx\"\n",
    "df = pd.read_excel(news_path)['Article']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "news_path = \"./0_newsInput/\"\n",
    "create_news(df, output_dir=news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "for file in listdir(news_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        news_list.append(file)\n",
    "news_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_news_list = news_list[START_IDX:END_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = create_newsdict(news_list=chosen_news_list, news_path=news_path)\n",
    "news_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Input Ontology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_onto_path = \"./4_refineOutput/RefinedUnifiedOntology.ttl\"\n",
    "filtered_onto_path = \"./5_filteredOnto/FilteredUnifiedOntology.ttl\"\n",
    "\n",
    "if path.exists(refine_onto_path):\n",
    "    prep_onto(refine_onto_path, filtered_onto_path)\n",
    "    with open(filtered_onto_path, 'r') as file:\n",
    "        initial_ontology = file.read()\n",
    "        file.close()\n",
    "    \n",
    "else:\n",
    "    initial_ontology = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompts Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt_text = \"\"\"I will provide you with a news article, and I want to generate an ontology from it. \n",
    "Please extract key concepts, relationships, and categories from the article and structure them into an ontology. \n",
    "The ontology should be in a structured format of Turtle (.ttl). \n",
    "Here is the news content: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"I will provide you with a news article, and I want to expand upon an existing ontology. \n",
    "Please analyze the new article, extract key concepts, relationships, and categories, and integrate them into the existing ontology while maintaining consistency and avoiding redundancy. \n",
    "Ensure that new concepts complement the previous ontology rather than duplicating existing ones.\n",
    "Only include the instances and properties essential for this specific news. \n",
    "Here is the news content: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt to LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_onto_DIR = \"./1_ontologiesInput/\"\n",
    "connect_output_DIR = \"./2_connectOutput/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear directories for new inputs\n",
    "\n",
    "\n",
    "if path.exists(input_onto_DIR):\n",
    "    for filename in listdir(input_onto_DIR):\n",
    "        file_path = path.join(input_onto_DIR, filename)\n",
    "        \n",
    "        if path.isfile(file_path) and filename != \".gitkeep\":\n",
    "            copy(file_path, \"./codes/news_onto/\")  \n",
    "            remove(file_path)  \n",
    "\n",
    "if path.exists(connect_output_DIR):\n",
    "    for filename in listdir(connect_output_DIR):\n",
    "        file_path = path.join(connect_output_DIR, filename)\n",
    "        \n",
    "        if path.isfile(file_path) and filename != \".gitkeep\":\n",
    "            remove(file_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoDict, responseList, promptList = generate_onto(URL, TOKEN, MODEL, \n",
    "                                                   initial_prompt_text=initial_prompt_text, \n",
    "                                                   prompt_text=prompt_text, \n",
    "                                                   news_dict=news_dict,\n",
    "                                                   initial_ontology=initial_ontology,\n",
    "                                                   output_dir=input_onto_DIR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ontology Merge Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilenames(input_path, extension=''):\n",
    "    return [f for f in next(walk(input_path), (None, None, []))[2] if f.endswith('.ttl')]  # [] if no file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_output_DIR = \"./3_mergeOutput/\"\n",
    "refine_output_DIR = \"./4_refineOutput/\"\n",
    "input = getFilenames(input_onto_DIR, '.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Every files in {input_path} will be updated with a new class,\n",
    "and an individual from said class.\n",
    "The class is named \"CyberNews\", and the individual is named after the file's name.\n",
    "Every other individuals in the ontology will have object property of \"relatedTo\" to said individual. \n",
    "\"\"\"\n",
    "\n",
    "connected = []\n",
    "news_class = \"CybersecurityNewsArticle\"\n",
    "\n",
    "for ind, file in enumerate(input):\n",
    "    if stat(input_onto_DIR + file).st_size == 0: # skip empty files\n",
    "        continue\n",
    "    g = Graph()\n",
    "    process_lines(f\"{input_onto_DIR}{file}\", f\"{input_onto_DIR}{file}\")\n",
    "    g.parse(f\"{input_onto_DIR}{file}\", format=\"ttl\")\n",
    "\n",
    "    prefix, uri = getNamespace(g)\n",
    "    news_name = file.split('.')[0]  # Get filename, exclude extension\n",
    "    connectIndividuals(g, prefix, uri, news_name, news_class)   \n",
    "\n",
    "    OutputDes = f\"{connect_output_DIR}{news_name}_connected.ttl\"  # Get new files' names\n",
    "    connected.append(OutputDes)\n",
    "    clear_serialize(g, OutputDes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_file = f\"./{merge_output_DIR}UnifiedOntology.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, connected_file in enumerate(connected):\n",
    "    append_file(source_file=connected_file, target_file=merged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse(merged_file, format=\"ttl\")\n",
    "clear_serialize(g, merged_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ontology Refinement Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UnifiedOntologyPipeline.tool.extractClass import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "loginHuggingFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_onto = \"./3_mergeOutput/UnifiedOntology.ttl\"\n",
    "df = getOntoClasses(path_to_onto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokenizer = \"bert-base-uncased\"\n",
    "name_model = \"OhWayTee/bert-taxonomy\"\n",
    "name_pipeline = \"text-classification\"\n",
    "\n",
    "classifier = getClassifier(name_tokenizer, name_model, name_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = getPredictions(classifier, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = sortPredictions(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_superclass = df_top[(df_top['label'] == \"LABEL_1\") & (df_top['score'] >= confidence_threshold)] # classB is SUBCLASS of classA\n",
    "df_subclass = df_top[(df_top['label'] == \"LABEL_2\") & (df_top['score'] >= confidence_threshold)] # classA is SUBCLASS of classB\n",
    "[df_superclass.shape, df_subclass.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_superclass_filtered = df_superclass.sort_values('score', ascending=False).drop_duplicates('classB') # remove duplicate subclasses\n",
    "df_subclass_filtered = df_subclass.sort_values('score', ascending=False).drop_duplicates('classA')\n",
    "[df_superclass_filtered.shape, df_subclass_filtered.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse(path_to_onto, format='turtle')\n",
    "prefix, uri = getNamespace(g)\n",
    "print(prefix, uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_superclass_filtered.iterrows():\n",
    "    query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX {prefix}: <{uri}>\n",
    "    INSERT{{\n",
    "        ?subclass rdfs:subClassOf ?superclass .\n",
    "    }}\n",
    "    WHERE {{\n",
    "        ?subclass rdfs:label \"{row['classB']}\" .\n",
    "        ?superclass rdfs:label \"{row['classA']}\" .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    g.update(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_subclass_filtered.iterrows():\n",
    "    query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX {prefix}: <{uri}>\n",
    "    INSERT{{\n",
    "        ?subclass rdfs:subClassOf ?superclass .\n",
    "    }}\n",
    "    WHERE {{\n",
    "        ?subclass rdfs:label \"{row['classA']}\" .\n",
    "        ?superclass rdfs:label \"{row['classB']}\" .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    g.update(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_onto_path = \"./4_refineOutput/RefinedUnifiedOntology.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_serialize(g, refine_onto_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering Ontology for Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.prep_onto import prep_onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_onto(refine_onto_path, filtered_onto_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
