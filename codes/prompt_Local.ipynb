{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(\".env\")\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ.get(\"APIKEY\")\n",
    "WEBUI_URL = \"http://localhost:8080\"\n",
    "MODEL = \"deepseek-r1:1.5b\"\n",
    "COLLECTION_ID = \"3e0b8bde-f5f9-4fb6-97f0-99ef34565d56\"\n",
    "TEST_ID = \"cfb8ddce-cba9-4bd4-ad47-60107c85a80c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./TheHackerNews_Dataset.xlsx\")['Article']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news(df, limit=20):\n",
    "    news_list = []\n",
    "    for idx, news in enumerate(df):\n",
    "        news_list.append(f\"news{idx + 1}.txt\")\n",
    "        if idx == limit - 1:\n",
    "            break\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query, file_id=None, collection_id=None):\n",
    "    url = f'{WEBUI_URL}/api/chat/completions'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    files = []\n",
    "\n",
    "    if file_id:\n",
    "        files.extend({'type': 'file', 'id': id} for id in file_id)\n",
    "    if collection_id:\n",
    "        files.extend({'type': 'collection', 'id': id} for id in collection_id)\n",
    "        \n",
    "    payload = {\n",
    "        'model': MODEL,\n",
    "        'messages': [{'role': 'user', 'content': query}],\n",
    "        'stream' : False,\n",
    "    }\n",
    "\n",
    "    if files:\n",
    "        payload['files'] = files\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return json.loads(response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_path):\n",
    "    url = f'{WEBUI_URL}/api/v1/files/'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    with open(file_path, 'rb') as f:\n",
    "        files = {'file': f}\n",
    "        response = requests.post(url, headers=headers, files=files)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uploaded_files():\n",
    "    url = f'{WEBUI_URL}/api/v1/files/'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    uploaded_files = {file[\"filename\"]: file[\"id\"] for file in json.loads(response.text)}\n",
    "    return uploaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_knowledge(knowledge_id, file_id):\n",
    "    url = f'{WEBUI_URL}/api/v1/knowledge/{knowledge_id}/file/add'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {'file_id': file_id}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file_from_knowledge(knowledge_id, file_id):\n",
    "    url = f'{WEBUI_URL}/api/v1/knowledge/{knowledge_id}/file/remove'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {'file_id': file_id}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file_from_upload(file_id):\n",
    "    url = f'{WEBUI_URL}/api/v1/files/{file_id}'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files():\n",
    "    url = f'{WEBUI_URL}/api/v1/files/all'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_output(output):\n",
    "    cleaned_text = re.sub(r'<think>.*?</think>', '', output, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_turtle(output):\n",
    "    cleaned_text = re.search(r'```(?:ttl|turtle)(.*?)```', output, flags=re.DOTALL)\n",
    "    if cleaned_text is not None:\n",
    "        return cleaned_text.group(1).strip()\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onto(initial_prompt_text, prompt_text):\n",
    "    ontoList = {}\n",
    "    prompt = initial_prompt_text\n",
    "    for news, news_id in get_uploaded_files().items():\n",
    "            \n",
    "        response = chat(query=prompt, file_id=[news_id])\n",
    "        ttl_content = extract_turtle(response['choices'][0]['message']['content'])\n",
    "        \n",
    "        if ttl_content:  \n",
    "            ontoList[news] = ttl_content  \n",
    "            \n",
    "            prompt = f\"{prompt_text}\\n\\n```ttl\\n{ttl_content}\\n```\"\n",
    "        else:\n",
    "            print(f\"Warning: No ontology extracted for {news}\")\n",
    "\n",
    "    return ontoList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = remove_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = create_news(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in news_list:\n",
    "    if news not in get_uploaded_files():\n",
    "        file_path = f\"./news/{news}\"\n",
    "        uploaded = upload_file(file_path)\n",
    "        print(uploaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename, file_id in get_uploaded_files().items():\n",
    "#     knowledge = add_file_to_knowledge(COLLECTION_ID, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt_text = \"\"\"\n",
    "I have provided you with a news article, and I want to generate an ontology from it. Please extract key concepts, relationships, and categories from the article and structure them into an ontology. The ontology should be in a structured format of Turtle (.ttl).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "I have provided you with a news article, and I want to expand upon an existing ontology. Please analyze the new article, extract key concepts, relationships, and categories, and integrate them into the existing ontology while maintaining consistency and avoiding redundancy. Ensure that new concepts complement the previous ontology rather than duplicating existing ones. Here is the ontology: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoList = generate_onto(initial_prompt_text=initial_prompt_text, prompt_text=prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
