{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import import_ipynb\n",
    "from prep_onto import prep_onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(\".env\")\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ.get(\"APIKEY\")\n",
    "WEBUI_URL = \"https://api.deepseek.com\"\n",
    "MODEL = \"deepseek-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_news(df, limit=20, output_dir=\"./news/\"):\n",
    "    news_list = []\n",
    "    for idx, news in enumerate(df):\n",
    "        filename = f\"news{str(idx + 1).zfill(3)}.txt\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        content = news\n",
    "\n",
    "        with open(file_path, 'w+') as file:\n",
    "            file.write(content)\n",
    "\n",
    "        news_list.append(filename)\n",
    "        \n",
    "        if idx == limit - 1:\n",
    "            break\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query):\n",
    "    url = f'{WEBUI_URL}/chat/completions'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "        \n",
    "    payload = {\n",
    "        'model': MODEL,\n",
    "        'messages': [{'role': 'user', 'content': query}],\n",
    "        'stream' : False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return json.loads(response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'error': f\"{str(e)} {response}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_turtle(output):\n",
    "    cleaned_text = re.search(r'```(?:ttl|turtle)(.*?)```', output, flags=re.DOTALL)\n",
    "    if cleaned_text is not None:\n",
    "        return cleaned_text.group(1).strip()\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_newsdict(news_list, news_path=\"./news/\"):\n",
    "    news_dict = {} \n",
    "    \n",
    "    for news in news_list:\n",
    "        try:\n",
    "            with open(news_path + news, \"r\") as f:\n",
    "                content = f.read()  \n",
    "            news_dict[news] = content  \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {news} not found in path {news_path}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {news}: {e}\")\n",
    "    \n",
    "    return news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ontologies(filename, content, output_dir=\"./news_onto\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    new_filename = filename.replace(\".txt\", \".ttl\")\n",
    "    file_path = os.path.join(output_dir, new_filename)\n",
    "        \n",
    "    with open(file_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "        \n",
    "    print(f\"Saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onto(initial_prompt_text, prompt_text, news_dict, initial_ontology=None):\n",
    "    if not (initial_prompt_text and prompt_text and news_dict):\n",
    "         return {}\n",
    "\n",
    "    ontoList = {}       # {'newsXYZ' : 'RDF_ONTOLOGY'}\n",
    "    responseList = []   # Saving response for Debugging\n",
    "    promptList = []     # Saving prompt for Debugging\n",
    "    first_iteration = True # Flag to trigger initial prompt\n",
    "    prompt = None\n",
    "\n",
    "    for news, news_content in news_dict.items():\n",
    "        if first_iteration:\n",
    "            # First Prompt Construction\n",
    "            if initial_ontology:\n",
    "                # Initial prompt w/ initial ontology\n",
    "                prompt = f\"\"\"\n",
    "{prompt_text}\\n\n",
    "{news_content}\\n\n",
    "Here is the ontology:\\n\n",
    "```ttl\n",
    "{initial_ontology}\n",
    "```\n",
    "\"\"\"\n",
    "            else:\n",
    "                # Initial prompt w/o initial ontology\n",
    "                prompt = initial_prompt_text + news_content\n",
    "            first_iteration = False\n",
    "\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "{prompt_text}\\n\n",
    "{news_content}\\n\n",
    "Here is the ontology:\\n\n",
    "```ttl\n",
    "{previous_ttl_content}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        # Send Prompt to LLM\n",
    "        response = chat(query=prompt)\n",
    "        \n",
    "        # Debug variables\n",
    "        promptList.append(prompt)\n",
    "        responseList.append(response)\n",
    "\n",
    "        # Get the generated ontology\n",
    "        ttl_content = extract_turtle(response['choices'][0]['message']['content'])\n",
    "        if ttl_content:  \n",
    "            ontoList[news] = ttl_content  \n",
    "            save_ontologies(news, ttl_content)\n",
    "            previous_ttl_content = ttl_content\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: No ontology extracted for {news}\")\n",
    "\n",
    "    return ontoList, responseList, promptList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./TheHackerNews_Dataset.xlsx\")['Article']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "create_news(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt_text = \"\"\"I will provide you with a news article, and I want to generate an ontology from it. \n",
    "Please extract key concepts, relationships, and categories from the article and structure them into an ontology. \n",
    "The ontology should be in a structured format of Turtle (.ttl). \n",
    "Here is the news content: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"I will provide you with a news article, and I want to expand upon an existing ontology. \n",
    "Please analyze the new article, extract key concepts, relationships, and categories, and integrate them into the existing ontology while maintaining consistency and avoiding redundancy. \n",
    "Ensure that new concepts complement the previous ontology rather than duplicating existing ones.\n",
    "Only include the instances and properties essential for this specific news. \n",
    "Here is the news content: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = \"./news/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "for file in os.listdir(news_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        news_list.append(file)\n",
    "news_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = create_newsdict(news_list=news_list[5:10], news_path=news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_onto_path = \"./uni_onto/RefinedUnifiedOntology.ttl\"\n",
    "if os.path.exists(ini_onto_path):\n",
    "    with open(ini_onto_path, 'r') as file:\n",
    "        initial_ontology = file.read()\n",
    "        file.close()\n",
    "else:\n",
    "    initial_ontology = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoList, responseList, promptList = generate_onto(initial_prompt_text=initial_prompt_text, \n",
    "                                                   prompt_text=prompt_text, \n",
    "                                                   news_dict=news_dict,\n",
    "                                                   initial_ontology=initial_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
