{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "from shutil import copy\n",
    "import pandas as pd\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(\"../../deepseek_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(project_dir)\n",
    "    print(\"Changed working directory to:\", os.getcwd())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory {project_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.prompt_API import create_news, create_newsdict, chat, extract_turtle, save_ontologies\n",
    "from UnifiedOntologyPipeline.pipeline.appendOnto import append_file, process_lines\n",
    "from UnifiedOntologyPipeline.pipeline.connectIndividuals import getNamespace, connectIndividuals, clear_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"./codes/.env\")\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ.get(\"APIKEY\")\n",
    "URL = \"https://api.deepseek.com\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "START_IDX = int(os.environ.get(\"NEWS_START\"))\n",
    "END_IDX = int(os.environ.get(\"NEWS_END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **News Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = \"./codes/TheHackerNews_Dataset.xlsx\"\n",
    "df = pd.read_excel(news_path)['Article']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "news_path = \"./0_newsInput/\"\n",
    "create_news(df, output_dir=news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "for file in os.listdir(news_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        news_list.append(file)\n",
    "news_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_news_list = news_list[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_news_list = news_list[START_IDX:END_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = create_newsdict(news_list=chosen_news_list, news_path=news_path)\n",
    "news_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompts Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt_text = \"\"\"I will provide you with a news article, and I want to generate an ontology from it. \n",
    "Please extract key concepts, relationships, and categories from the article and structure them into an ontology. \n",
    "The ontology should be in a structured format of Turtle (.ttl). Use \"@prefix ex: <http://example.org/ontology#>\". \n",
    "Here is the news content: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt to LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_onto_DIR = \"./baseline/1_ontologiesInputBL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoDict = {}\n",
    "promptList = []\n",
    "responseList = []\n",
    "\n",
    "for news, news_content in news_dict.items():\n",
    "    prompt = f\"\"\"\n",
    "    {initial_prompt_text}\n",
    "    {news_content}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Send Prompt to LLM\n",
    "        response = chat(prompt, URL, TOKEN, MODEL)\n",
    "        \n",
    "        # Debug variables\n",
    "        promptList.append(prompt)\n",
    "        responseList.append(response)\n",
    "        \n",
    "        ttl_content = extract_turtle(response['choices'][0]['message']['content'])\n",
    "        if ttl_content:\n",
    "            ontoDict[news] = ttl_content\n",
    "            save_ontologies(news, ttl_content, output_dir=input_onto_DIR)\n",
    "    except Exception as e:\n",
    "        ontoDict[news] = str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ontology Merge Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilenames(input_path, extension=''):\n",
    "    return [f for f in next(os.walk(input_path), (None, None, []))[2] if f.endswith('.ttl')]  # [] if no file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_output_DIR = \"./baseline/2_connectOutputBL/\"\n",
    "merge_output_DIR = \"./baseline/3_mergeOutputBL/\"\n",
    "refine_output_DIR = \"./baseline/4_refineOutputBL/\"\n",
    "input = getFilenames(input_onto_DIR, '.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Every files in {input_path} will be updated with a new class,\n",
    "and an individual from said class.\n",
    "The class is named \"CyberNews\", and the individual is named after the file's name.\n",
    "Every other individuals in the ontology will have object property of \"relatedTo\" to said individual. \n",
    "\"\"\"\n",
    "\n",
    "connected = []\n",
    "news_class = \"CybersecurityNewsArticle\"\n",
    "\n",
    "for ind, file in enumerate(input):\n",
    "    if os.stat(input_onto_DIR + file).st_size == 0: # skip empty files\n",
    "        continue\n",
    "    g = Graph()\n",
    "    process_lines(f\"{input_onto_DIR}{file}\", f\"{input_onto_DIR}{file}\")\n",
    "    g.parse(f\"{input_onto_DIR}{file}\", format=\"ttl\")\n",
    "\n",
    "    prefix, uri = getNamespace(g)\n",
    "    news_name = file.split('.')[0]  # Get filename, exclude extension\n",
    "    connectIndividuals(g, prefix, uri, news_name, news_class)   \n",
    "\n",
    "    OutputDes = f\"{connect_output_DIR}{news_name}_connected.ttl\"  # Get new files' names\n",
    "    connected.append(OutputDes)\n",
    "    clear_serialize(g, OutputDes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_file = f\"./{merge_output_DIR}UnifiedOntology.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, connected_file in enumerate(connected):\n",
    "    append_file(source_file=connected_file, target_file=merged_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse(merged_file, format=\"ttl\")\n",
    "clear_serialize(g, merged_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ontology Refinement Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UnifiedOntologyPipeline.tool.extractClass import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "loginHuggingFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_onto = \"./3_mergeOutput/UnifiedOntology.ttl\"\n",
    "df = getOntoClasses(path_to_onto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tokenizer = \"bert-base-uncased\"\n",
    "name_model = \"OhWayTee/bert-taxonomy\"\n",
    "name_pipeline = \"text-classification\"\n",
    "\n",
    "classifier = getClassifier(name_tokenizer, name_model, name_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = getPredictions(classifier, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = sortPredictions(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_superclass = df_top[(df_top['label'] == \"LABEL_1\") & (df_top['score'] >= confidence_threshold)] # classB is SUBCLASS of classA\n",
    "df_subclass = df_top[(df_top['label'] == \"LABEL_2\") & (df_top['score'] >= confidence_threshold)] # classA is SUBCLASS of classB\n",
    "[df_superclass.shape, df_subclass.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_superclass_filtered = df_superclass.sort_values('score', ascending=False).drop_duplicates('classB') # remove duplicate subclasses\n",
    "df_subclass_filtered = df_subclass.sort_values('score', ascending=False).drop_duplicates('classA')\n",
    "[df_superclass_filtered.shape, df_subclass_filtered.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse(path_to_onto, format='turtle')\n",
    "prefix, uri = getNamespace(g)\n",
    "print(prefix, uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_superclass_filtered.iterrows():\n",
    "    query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX {prefix}: <{uri}>\n",
    "    INSERT{{\n",
    "        ?subclass rdfs:subClassOf ?superclass .\n",
    "    }}\n",
    "    WHERE {{\n",
    "        ?subclass rdfs:label \"{row['classB']}\" .\n",
    "        ?superclass rdfs:label \"{row['classA']}\" .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    g.update(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_subclass_filtered.iterrows():\n",
    "    query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX {prefix}: <{uri}>\n",
    "    INSERT{{\n",
    "        ?subclass rdfs:subClassOf ?superclass .\n",
    "    }}\n",
    "    WHERE {{\n",
    "        ?subclass rdfs:label \"{row['classA']}\" .\n",
    "        ?superclass rdfs:label \"{row['classB']}\" .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    g.update(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_onto_path = \"./baseline/4_refineOutputBL/RefinedUnifiedOntology.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_serialize(g, refine_onto_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
